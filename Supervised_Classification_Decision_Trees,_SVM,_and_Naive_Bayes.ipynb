{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1.What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "In the context of decision trees, Information Gain is a measure of how much 'purity' or 'order' a split brings to a dataset. It quantifies the reduction in entropy (or uncertainty) after a dataset is split based on an attribute. The higher the information gain, the better the split, as it means the attribute effectively separates the data into more homogeneous groups.\n",
        "\n",
        "It's calculated using the concept of Entropy, which is a measure of randomness or impurity in a set of examples. A dataset with high entropy is very mixed, while a dataset with low entropy is more homogeneous.\n",
        "\n",
        "How is it used in Decision Trees?\n",
        "\n",
        "- Decision Trees are built by recursively splitting the dataset into subsets based on the values of attributes. At each step, the algorithm needs to decide which attribute to split on. This is where Information Gain comes in:\n",
        "\n",
        "- Select the Best Split: For every potential split (i.e., for every attribute and every possible value of that attribute), the decision tree algorithm calculates the Information Gain. The attribute that yields the highest Information Gain is chosen as the splitting criterion for the current node.\n",
        "\n",
        "- Maximize Purity: The goal is to choose splits that result in child nodes that are as 'pure' as possible, meaning they contain instances predominantly belonging to a single class. Information Gain helps achieve this by favoring attributes that reduce the entropy the most.\n",
        "\n",
        "- Recursive Process: This process is repeated for each new child node until a stopping criterion is met (e.g., all instances in a node belong to the same class, no more attributes to split on, or a maximum tree depth is reached)."
      ],
      "metadata": {
        "id": "KvGX_fv8imJO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "\n",
        "Gini Impurity and Entropy are both impurity measures for decision trees; they are mathematically different but usually give very similar splits in practice. Gini is slightly simpler and faster to compute, while Entropy has a stronger grounding in information theory and can give marginally more balanced splits.​\n",
        "\n",
        "\n",
        "Definitions\n",
        "\n",
        "- Gini Impurity measures how often a randomly chosen sample from a node would be misclassified if its label were assigned randomly according to the node’s class proportions.\n",
        "\n",
        "- Entropy measures the uncertainty or disorder in the class distribution of a node, coming directly from information theory.\n",
        "\n",
        "Behavior and numeric range\n",
        "\n",
        "- Both reach 0 when the node is pure (all samples in one class), and both are higher when classes are mixed. For binary classification, Entropy ranges from 0 to 1, while Gini ranges from 0 to 0.5, but their curves with respect to class probability are very similar.​\n",
        "\n",
        "- Entropy is slightly more sensitive to changes near the extremes (e.g., class probabilities 0.1 vs 0.2), whereas Gini behaves more linearly in that region. In practice this means Entropy may react a bit more to subtle changes in minority-class probability, though the effect on real trees is often small.​\n",
        "\n",
        "Computational cost and split tendencies\n",
        "\n",
        "- Gini uses only squares and sums, so it is cheaper to compute than Entropy, which involves logarithms; this matters when building many trees or training on large, high‑dimensional datasets. Many libraries (e.g., CART-style implementations) therefore default to Gini for speed.​\n",
        "\n",
        "- Empirical studies show both criteria usually pick similar or identical best splits, but there are nuances: Gini tends to favor splits that isolate the majority class quickly, while Entropy more often produces balanced partitions of the classes.​\n",
        "\n",
        "Strengths, weaknesses, and when to use\n",
        "\n",
        "1. Gini Impurity\n",
        "\n",
        "- Strengths: Faster, simpler, usually performs as well as Entropy; good default, especially for large datasets or ensembles like random forests.​\n",
        "\n",
        "- Weaknesses: Slight bias toward the majority class when choosing splits; slightly less theoretically interpretable from an information theory standpoint.​\n",
        "\n",
        "2. Entropy\n",
        "\n",
        "- Strengths: Strong information-theoretic grounding (used with Information Gain); can give slightly more balanced splits and is more sensitive to subtle probability differences.​\n",
        "\n",
        "- Weaknesses: More expensive to compute due to logarithms; in many practical tasks, accuracy gains over Gini are negligible.​\n",
        "\n",
        "Practical guideline\n",
        "\n",
        "- If optimizing for training speed or using large/random-forest-style models, choose Gini as the default.​\n",
        "\n",
        "- If focusing on theoretical clarity (e.g., teaching, research) or wanting a criterion explicitly tied to information gain, choose Entropy; expect trees and metrics to be very similar to Gini in most real-world datasets."
      ],
      "metadata": {
        "id": "18xUMYn1jKPP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Pre-pruning (also called early stopping) is a strategy where you stop growing a decision tree before it becomes fully grown, using constraints or stopping rules to avoid overfitting. Instead of first building a large tree and then cutting it back, pre-pruning makes the tree stay small from the beginning.\n",
        "\n",
        "Core idea\n",
        "\n",
        "- In pre-pruning, you modify the tree-building algorithm so that it refuses to split a node when a chosen condition is not met (e.g., not enough samples, too little impurity decrease, no significant validation improvement).​\n",
        "\n",
        "- The goal is to prevent overly deep, highly specific trees that memorize training noise, improving generalization and keeping models simpler and faster."
      ],
      "metadata": {
        "id": "cmj43BBRkT-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.Write a Python program to train a Decision Tree Classifier using Gini Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "sP2iAGmik3iP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 1. Generate a synthetic dataset\n",
        "X, y = make_classification(\n",
        "    n_samples=1000,          # 1000 samples\n",
        "    n_features=10,           # 10 features\n",
        "    n_informative=5,         # 5 informative features\n",
        "    n_redundant=2,           # 2 redundant features\n",
        "    n_repeated=0,            # 0 repeated features\n",
        "    n_classes=2,             # 2 classes\n",
        "    random_state=42          # for reproducibility\n",
        ")\n",
        "\n",
        "# For better readability of feature importances, let's name the features\n",
        "feature_names = [f'feature_{i}' for i in range(X.shape[1])]\n",
        "\n",
        "# 2. Split data into training and testing sets (optional for just printing importances, but good practice)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 3. Initialize the Decision Tree Classifier with Gini Impurity\n",
        "#    criterion='gini' is the default, but explicitly setting it highlights the point.\n",
        "dtc = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "\n",
        "# 4. Train the classifier\n",
        "dtc.fit(X_train, y_train)\n",
        "\n",
        "# 5. Print feature importances\n",
        "print(\"Feature Importances (Gini Impurity):\")\n",
        "for i, importance in enumerate(dtc.feature_importances_):\n",
        "    print(f\"{feature_names[i]}: {importance:.4f}\")\n",
        "\n",
        "# You can also get a sorted list\n",
        "sorted_importances = sorted(zip(feature_names, dtc.feature_importances_), key=lambda x: x[1], reverse=True)\n",
        "print(\"\\nSorted Feature Importances (Gini Impurity):\")\n",
        "for name, importance in sorted_importances:\n",
        "    print(f\"{name}: {importance:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5gkSAdblJCi",
        "outputId": "ba0b8cce-e416-4179-ec1a-de4a28bfbe77"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances (Gini Impurity):\n",
            "feature_0: 0.2840\n",
            "feature_1: 0.0093\n",
            "feature_2: 0.0090\n",
            "feature_3: 0.0735\n",
            "feature_4: 0.2351\n",
            "feature_5: 0.2463\n",
            "feature_6: 0.0676\n",
            "feature_7: 0.0127\n",
            "feature_8: 0.0049\n",
            "feature_9: 0.0577\n",
            "\n",
            "Sorted Feature Importances (Gini Impurity):\n",
            "feature_0: 0.2840\n",
            "feature_5: 0.2463\n",
            "feature_4: 0.2351\n",
            "feature_3: 0.0735\n",
            "feature_6: 0.0676\n",
            "feature_9: 0.0577\n",
            "feature_7: 0.0127\n",
            "feature_1: 0.0093\n",
            "feature_2: 0.0090\n",
            "feature_8: 0.0049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5.What is a Support Vector Machine (SVM)?\n",
        "\n",
        "A Support Vector Machine (SVM) is a supervised learning algorithm that finds a decision boundary (hyperplane) that best separates classes by maximizing the margin between them. It is used mainly for classification but also has variants for regression (SVR) and anomaly detection (one‑class SVM).​\n",
        "\n",
        "\n",
        "Main variants and uses\n",
        "\n",
        "- Classification: Standard binary SVM; multi‑class problems are handled via one‑vs‑rest or one‑vs‑one schemes.​\n",
        "\n",
        "- Regression (SVR): Learns a function with an “epsilon-insensitive” margin, focusing only on points lying outside a tolerance tube around the prediction.​\n",
        "\n",
        "- One‑class SVM: Learns the support of a single “normal” class for anomaly or outlier detection"
      ],
      "metadata": {
        "id": "9_Oh7SFClQwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6.What is the Kernel Trick in SVM?\n",
        "\n",
        "The kernel trick is a way to make SVMs learn non‑linear decision boundaries by using a special similarity function (kernel) instead of explicitly mapping data into a high‑dimensional feature space. It lets the algorithm behave as if it is doing linear separation in a higher dimension, without ever computing those high‑dimensional coordinates directly.\n",
        "\n",
        "Why this helps\n",
        "\n",
        "- By choosing a non‑linear kernel, the SVM effectively separates data that are not linearly separable in the original space but become linearly separable after the implicit mapping.​\n",
        "\n",
        "- This gives SVMs powerful non‑linear decision boundaries while keeping the optimization problem convex and computationally feasible, since only kernel evaluations\n",
        "K\n",
        "(\n",
        "x\n",
        "i\n",
        ",\n",
        "x\n",
        "j\n",
        ")\n",
        " are needed."
      ],
      "metadata": {
        "id": "6Sak5wrQlpnj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7.Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then compare their accuracies.\n"
      ],
      "metadata": {
        "id": "d3rx-gx9mAHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize SVM classifiers with different kernels\n",
        "# Linear Kernel SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "\n",
        "# RBF Kernel SVM\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "\n",
        "# 4. Train both classifiers\n",
        "print(\"Training Linear Kernel SVM...\")\n",
        "svm_linear.fit(X_train, y_train)\n",
        "print(\"Training RBF Kernel SVM...\")\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred_linear = svm_linear.predict(X_test)\n",
        "y_pred_rbf = svm_rbf.predict(X_test)\n",
        "\n",
        "# 6. Calculate and compare accuracy scores\n",
        "accuracy_linear = accuracy_score(y_test, y_pred_linear)\n",
        "accuracy_rbf = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(f\"\\nAccuracy with Linear Kernel SVM: {accuracy_linear:.4f}\")\n",
        "print(f\"Accuracy with RBF Kernel SVM:   {accuracy_rbf:.4f}\")\n",
        "\n",
        "# Provide a conclusion based on the comparison\n",
        "if accuracy_linear > accuracy_rbf:\n",
        "    print(\"\\nConclusion: The Linear Kernel SVM performed better on this dataset.\")\n",
        "elif accuracy_rbf > accuracy_linear:\n",
        "    print(\"\\nConclusion: The RBF Kernel SVM performed better on this dataset.\")\n",
        "else:\n",
        "    print(\"\\nConclusion: Both Linear and RBF Kernel SVMs performed equally well on this dataset.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Suw3H0c7mJWe",
        "outputId": "b5d7cd85-7b7c-4947-9a63-4fbbf004ad87"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Linear Kernel SVM...\n",
            "Training RBF Kernel SVM...\n",
            "\n",
            "Accuracy with Linear Kernel SVM: 0.9815\n",
            "Accuracy with RBF Kernel SVM:   0.7593\n",
            "\n",
            "Conclusion: The Linear Kernel SVM performed better on this dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8.What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "\n",
        "Naïve Bayes is a family of supervised probabilistic classifiers that use Bayes’ theorem to predict the most likely class for a given feature vector. It is called “naïve” because it makes a very strong (and usually unrealistic) assumption that all features are conditionally independent of each other given the class label.\n",
        "\n",
        "Why it is called “Naïve”\n",
        "\n",
        "- The method assumes that, once the class is known, each feature provides information independent of all the other features, i.e., features have no direct influence on each other given the class.​\n",
        "\n",
        "- This “naïve independence assumption” is rarely true in real data, hence the name; nonetheless, the classifier often works surprisingly well in practice, especially for text classification, spam filtering, and sentiment analysis.​\n",
        "\n",
        "Key properties and uses\n",
        "\n",
        "- Strengths: very simple, fast to train and predict, works well with high-dimensional and sparse data, and needs relatively little training data.​\n",
        "\n",
        "- Common applications include spam detection, document/topic classification, sentiment analysis, and other problems where feature counts (like word frequencies) are used as inputs.​"
      ],
      "metadata": {
        "id": "AyvTWe3UmRXU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes.\n",
        "\n",
        "Gaussian, Multinomial, and Bernoulli Naïve Bayes are all the same Naïve Bayes idea with different assumptions about how features are distributed, so each suits a different data type. Gaussian is for continuous features, Multinomial for count-based discrete features, and Bernoulli for binary (0/1) features.​\n",
        "\n",
        "Gaussian Naïve Bayes\n",
        "\n",
        "- Assumption: Each feature is continuous and, within each class, follows a Gaussian (normal) distribution; for feature\n",
        "x\n",
        "i\n",
        "x\n",
        "i\n",
        "  in class\n",
        "y\n",
        "y, the likelihood\n",
        "P\n",
        "(\n",
        "x\n",
        "i\n",
        "∣\n",
        "y\n",
        ")\n",
        "P(x\n",
        "i\n",
        " ∣y) is modeled by a normal distribution with class-specific mean and variance.​\n",
        "\n",
        "- Typical use: Numeric, continuous data such as sensor readings, measurements (e.g., Iris dataset: petal/sepal lengths and widths), or any real‑valued features.​\n",
        "\n",
        "Multinomial Naïve Bayes\n",
        "\n",
        "- Assumption: Features are non‑negative integer counts (or proportions) following a multinomial distribution; the likelihood models how often each discrete outcome (e.g., word) occurs in a sample.​\n",
        "\n",
        "- Typical use: Text classification with bag‑of‑words/tf counts (spam detection, topic classification), or any setting where features represent event counts per instance.​\n",
        "\n",
        "Bernoulli Naïve Bayes\n",
        "\n",
        "- Assumption: Each feature is binary (0/1), indicating presence or absence, and follows a Bernoulli distribution for each class.​\n",
        "\n",
        "- Typical use: Text or other data where you care about whether a feature occurs at all, not how many times (e.g., “word present vs not present” per document, yes/no flags, on/off indicators)."
      ],
      "metadata": {
        "id": "Je95fmlvmh1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10.Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "IlGeQKyim4Uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1. Load the Breast Cancer dataset\n",
        "cancer = load_breast_cancer()\n",
        "X = cancer.data\n",
        "y = cancer.target\n",
        "\n",
        "# 2. Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# 3. Initialize the Gaussian Naïve Bayes classifier\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# 4. Train the classifier\n",
        "print(\"Training Gaussian Naïve Bayes Classifier...\")\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# 5. Make predictions on the test set\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# 6. Calculate and print the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f\"\\nAccuracy of Gaussian Naïve Bayes Classifier: {accuracy:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "znhGDdhwnV4Z",
        "outputId": "16441ad4-4295-4d3c-cbc0-1cbaa7eac464"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Gaussian Naïve Bayes Classifier...\n",
            "\n",
            "Accuracy of Gaussian Naïve Bayes Classifier: 0.9415\n"
          ]
        }
      ]
    }
  ]
}